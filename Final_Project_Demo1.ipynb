{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Thư viện"
      ],
      "metadata": {
        "id": "xhubLw0bu_Gt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A61sdOBMubev"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Tuple\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cấu hình"
      ],
      "metadata": {
        "id": "Qcr0o5LhwSRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "  #data_path: str = '/mnt/data/Metro_Interstate_Traffic_Volume.csv' # đường dẫn file (tùy chỉnh khi khởi tạo)\n",
        "  data_path: str = '/content/Metro_Interstate_Traffic_Volume.csv'\n",
        "  pred_col: str = 'traffic_volume'\n",
        "  features: List[str] = None # nếu None sẽ tự động chọn numeric + time features\n",
        "  lookback: int = 24\n",
        "  horizon_task1: int = 2\n",
        "  horizon_task2: int = 5\n",
        "  test_size_ratio: float = 0.15\n",
        "  val_size_ratio: float = 0.15\n",
        "  batch_size: int = 64\n",
        "  scaler_path: str = '/content/data/processed/scaler.save'\n",
        "  cleaned_data_path: str = '/content/data/processed/cleaned_data.parquet'\n",
        "  models_dir: str = '/content/data/models'\n",
        "  device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  def __post_init__(self):\n",
        "    if self.features is None:\n",
        "    # default numeric features plus time\n",
        "      self.features = ['traffic_volume', 'temp', 'rain_1h', 'snow_1h', 'clouds_all', 'hour', 'dayofweek']"
      ],
      "metadata": {
        "id": "pCVlD05DvHyn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xử lí dữ liệu thô"
      ],
      "metadata": {
        "id": "AXm5xz8AwWXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Data processing OOP\n",
        "# ----------------------------\n",
        "class DataProcessor:\n",
        "  \"\"\"\n",
        "      Class chịu trách nhiệm đọc, clean, feature-engineer, scale và xuất dữ liệu đã xử lý.\n",
        "      Giao diện rõ ràng: khởi tạo với Config, gọi .run() để thực hiện và lưu file.\n",
        "  \"\"\"\n",
        "  def __init__(self, cfg: Config):\n",
        "    self.cfg = cfg\n",
        "    os.makedirs(os.path.dirname(cfg.cleaned_data_path), exist_ok=True)\n",
        "    os.makedirs(cfg.models_dir, exist_ok=True)\n",
        "    self.scaler = None\n",
        "\n",
        "  def read(self) -> pd.DataFrame:\n",
        "    df = pd.read_csv(self.cfg.data_path)\n",
        "    return df\n",
        "\n",
        "  def clean_and_feature(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "  # Kiểm tra cột datetime\n",
        "    if 'date_time' not in df.columns:\n",
        "      raise ValueError('Dữ liệu cần cột date_time')\n",
        "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
        "    df = df.sort_values('date_time').reset_index(drop=True)\n",
        "    df = df.set_index('date_time')\n",
        "\n",
        "    # tạo time features\n",
        "    df['hour'] = df.index.hour\n",
        "    df['dayofweek'] = df.index.dayofweek\n",
        "    # điền missing cho các cột quan trọng\n",
        "    needed = [self.cfg.pred_col] + [c for c in self.cfg.features if c!=self.cfg.pred_col]\n",
        "    for c in needed:\n",
        "      if c not in df.columns:\n",
        "        raise ValueError(f'Column {c} không có trong dữ liệu')\n",
        "    df[needed] = df[needed].interpolate().fillna(method='bfill').fillna(method='ffill')\n",
        "    return df\n",
        "\n",
        "  def scale(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, MinMaxScaler]:\n",
        "    cols = self.cfg.features\n",
        "    self.scaler = MinMaxScaler()\n",
        "    arr = self.scaler.fit_transform(df[cols])\n",
        "    df_scaled = pd.DataFrame(arr, index=df.index, columns=cols)\n",
        "    return df_scaled, self.scaler\n",
        "\n",
        "  def save(self, df_scaled: pd.DataFrame):\n",
        "  # lưu parquet cho nhanh, lưu scaler\n",
        "    df_scaled.to_parquet(self.cfg.cleaned_data_path, index=True)\n",
        "    joblib.dump(self.scaler, self.cfg.scaler_path)\n",
        "\n",
        "  def run(self) -> pd.DataFrame:\n",
        "    df = self.read()\n",
        "    df_clean = self.clean_and_feature(df)\n",
        "    df_scaled, scaler = self.scale(df_clean)\n",
        "    self.save(df_scaled)\n",
        "    print('Saved cleaned scaled data to', self.cfg.cleaned_data_path)\n",
        "    print('Saved scaler to', self.cfg.scaler_path)\n",
        "    return df_scaled\n",
        "\n",
        "  # test unit\n",
        "  def test_read_and_clean(self):\n",
        "    df = self.read()\n",
        "    df2 = self.clean_and_feature(df.copy())\n",
        "    print('Test read/clean: columns after processing ->', df2.columns.tolist())"
      ],
      "metadata": {
        "id": "wi9BtoLbwJ3d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "u9SpjyPLxyl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Dataset class (reusable)\n",
        "# ----------------------------\n",
        "class TimeSeriesDataset(Dataset):\n",
        "  def __init__(self, df: pd.DataFrame, lookback: int, horizon: int, pred_col: str):\n",
        "    self.array = df.values.astype(np.float32)\n",
        "    self.lookback = lookback\n",
        "    self.horizon = horizon\n",
        "    self.pred_idx = df.columns.get_loc(pred_col)\n",
        "    self.length = len(self.array) - lookback - horizon + 1\n",
        "\n",
        "  def __len__(self):\n",
        "    return max(0, self.length)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    s = idx\n",
        "    X = self.array[s:s+self.lookback]\n",
        "    y = self.array[s+self.lookback:s+self.lookback+self.horizon, self.pred_idx]\n",
        "    return torch.from_numpy(X), torch.from_numpy(y)"
      ],
      "metadata": {
        "id": "ydSOBbTkxi4p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mô hình"
      ],
      "metadata": {
        "id": "FAZiJScm1BRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Model definitions (each class is independent)\n",
        "# ----------------------------\n",
        "class SimpleRNN(nn.Module):\n",
        "  def __init__(self, input_dim:int, hidden:int=64, num_layers:int=1, horizon:int=1):\n",
        "    super().__init__()\n",
        "    self.rnn = nn.RNN(input_dim, hidden, num_layers=num_layers, batch_first=True)\n",
        "    self.head = nn.Sequential(nn.Linear(hidden, hidden//2), nn.ReLU(), nn.Linear(hidden//2, horizon))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, _ = self.rnn(x)\n",
        "    last = out[:, -1, :]\n",
        "    return self.head(last)\n",
        "\n",
        "class SimpleLSTM(nn.Module):\n",
        "  def __init__(self, input_dim:int, hidden:int=64, num_layers:int=1, horizon:int=1):\n",
        "    super().__init__()\n",
        "    self.lstm = nn.LSTM(input_dim, hidden, num_layers=num_layers, batch_first=True)\n",
        "    self.head = nn.Sequential(nn.Linear(hidden, hidden//2), nn.ReLU(), nn.Linear(hidden//2, horizon))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, _ = self.lstm(x)\n",
        "    last = out[:, -1, :]\n",
        "    return self.head(last)"
      ],
      "metadata": {
        "id": "ddlIMs6Fx2As"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Customized LSTM: thêm attention + residual connection\n",
        "class CustomizedLSTM(nn.Module):\n",
        "  def __init__(self, input_dim:int, hidden:int=128, num_layers:int=1, horizon:int=1):\n",
        "    super().__init__()\n",
        "    self.lstm = nn.LSTM(input_dim, hidden, num_layers=num_layers, batch_first=True, dropout=0.1)\n",
        "    self.attn = nn.Sequential(nn.Linear(hidden, hidden//2), nn.Tanh(), nn.Linear(hidden//2, 1))\n",
        "    self.res_fc = nn.Linear(input_dim, hidden)\n",
        "    self.head = nn.Sequential(nn.Linear(hidden, hidden//2), nn.ReLU(), nn.Linear(hidden//2, horizon))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: batch, seq, feat\n",
        "    out, _ = self.lstm(x) # out: batch, seq, hidden\n",
        "    # attention pooling over time\n",
        "    scores = self.attn(out).squeeze(-1) # batch, seq\n",
        "    weights = torch.softmax(scores, dim=1).unsqueeze(-1)\n",
        "    context = (out * weights).sum(dim=1) # batch, hidden\n",
        "    # residual from last input timestep features (projected)\n",
        "    residual = self.res_fc(x[:, -1, :])\n",
        "    combined = context + residual\n",
        "    return self.head(combined)"
      ],
      "metadata": {
        "id": "gdpsAKVlyPQY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seq2Seq with Attention (encoder-decoder) for multistep\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden):\n",
        "    super().__init__()\n",
        "    self.lstm = nn.LSTM(input_dim, hidden, batch_first=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, (h,c) = self.lstm(x)\n",
        "    return out, (h,c)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, hidden):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(hidden*2, hidden)\n",
        "    self.v = nn.Linear(hidden, 1, bias=False)\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs):\n",
        "    seq = encoder_outputs.size(1)\n",
        "    hidden = hidden.unsqueeze(1).repeat(1, seq, 1)\n",
        "    e = torch.tanh(self.fc(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "    s = self.v(e).squeeze(2)\n",
        "    w = torch.softmax(s, dim=1)\n",
        "    ctx = torch.bmm(w.unsqueeze(1), encoder_outputs).squeeze(1)\n",
        "    return ctx, w\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden):\n",
        "    super().__init__()\n",
        "    self.cell = nn.LSTMCell(input_dim, hidden)\n",
        "    self.att = Attention(hidden)\n",
        "    self.out = nn.Linear(hidden, 1)\n",
        "\n",
        "  def forward(self, enc_out, h, c, tgt_len):\n",
        "    batch = enc_out.size(0); device = enc_out.device\n",
        "    inp = torch.zeros(batch, 1, device=device)\n",
        "    h = h.squeeze(0); c = c.squeeze(0)\n",
        "    outs = []\n",
        "    for _ in range(tgt_len):\n",
        "      ctx, _ = self.att(h, enc_out)\n",
        "      rnn_in = torch.cat((inp, ctx), dim=1)\n",
        "      h, c = self.cell(rnn_in, (h, c))\n",
        "      o = self.out(h)\n",
        "      outs.append(o)\n",
        "      inp = o\n",
        "    outs = torch.stack(outs, dim=1).squeeze(-1)\n",
        "    return outs\n",
        "\n",
        "\n",
        "class Seq2SeqAttention(nn.Module):\n",
        "  def __init__(self, input_dim, enc_hidden=128, dec_hidden=128):\n",
        "    super().__init__()\n",
        "    self.enc = Encoder(input_dim, enc_hidden)\n",
        "    self.dec = Decoder(1+enc_hidden, dec_hidden)\n",
        "\n",
        "  def forward(self, x, tgt_len):\n",
        "    enc_out, (h,c) = self.enc(x)\n",
        "    return self.dec(enc_out, h, c, tgt_len)"
      ],
      "metadata": {
        "id": "00YWJpjnygHg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Trainer + Evaluator utilities\n",
        "# ----------------------------\n",
        "class Trainer:\n",
        "  def __init__(self, cfg: Config, scaler_path: str = None):\n",
        "    self.cfg = cfg\n",
        "    self.device = torch.device(cfg.device)\n",
        "    self.scaler = None\n",
        "    if scaler_path and os.path.exists(scaler_path):\n",
        "      self.scaler = joblib.load(scaler_path)\n",
        "\n",
        "  def _inv_scale(self, y_scaled: np.ndarray, scaler) -> np.ndarray:\n",
        "    n, h = y_scaled.shape\n",
        "    mats = np.zeros((n, len(self.cfg.features)))\n",
        "    outs = []\n",
        "    for i in range(h):\n",
        "      mats[:, self.cfg.features.index(self.cfg.pred_col)] = y_scaled[:, i]\n",
        "      outs.append(scaler.inverse_transform(mats)[:, self.cfg.features.index(self.cfg.pred_col)])\n",
        "    return np.stack(outs, axis=1)\n",
        "\n",
        "  def train(self, model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
        "    epochs=10, lr=1e-3, task=1, horizon=1):\n",
        "    model = model.to(self.device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    crit = nn.MSELoss()\n",
        "    best_state = None; best_val = float('inf')\n",
        "    for ep in range(1, epochs+1):\n",
        "      model.train(); tr_losses = []\n",
        "      for xb, yb in train_loader:\n",
        "        xb = xb.to(self.device); yb = yb.to(self.device)\n",
        "        opt.zero_grad()\n",
        "        if task==1:\n",
        "          out = model(xb)\n",
        "        else:\n",
        "          out = model(xb, horizon)\n",
        "        loss = crit(out, yb)\n",
        "        loss.backward(); opt.step(); tr_losses.append(loss.item())\n",
        "      model.eval(); val_losses = []\n",
        "      with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "          xb = xb.to(self.device); yb = yb.to(self.device)\n",
        "          if task==1:\n",
        "            out = model(xb)\n",
        "          else:\n",
        "            out = model(xb, horizon)\n",
        "          val_losses.append(crit(out, yb).item())\n",
        "      tr_loss = float(np.mean(tr_losses)); val_loss = float(np.mean(val_losses))\n",
        "      if val_loss < best_val:\n",
        "        best_val = val_loss; best_state = model.state_dict()\n",
        "      if ep % max(1, epochs//5) == 0 or ep==1:\n",
        "        print(f'Epoch {ep}/{epochs} train={tr_loss:.6f} val={val_loss:.6f}')\n",
        "    if best_state is not None:\n",
        "      model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "  def evaluate(self, model: nn.Module, loader: DataLoader, horizon:int, task=1, scaler=None):\n",
        "    model = model.to(self.device); model.eval()\n",
        "    Ys=[]; Ps=[]\n",
        "    with torch.no_grad():\n",
        "      for xb, yb in loader:\n",
        "        xb = xb.to(self.device)\n",
        "        if task==1:\n",
        "          p = model(xb).cpu().numpy()\n",
        "        else:\n",
        "          p = model(xb, horizon).cpu().numpy()\n",
        "        Ys.append(yb.numpy()); Ps.append(p)\n",
        "    Y = np.vstack(Ys); P = np.vstack(Ps)\n",
        "    if scaler is None:\n",
        "      raise ValueError('scaler required to inverse transform')\n",
        "    Y_inv = self._inv_scale(Y, scaler); P_inv = self._inv_scale(P, scaler)\n",
        "    rows = []\n",
        "    for h in range(horizon):\n",
        "      yt = Y_inv[:, h]; yp = P_inv[:, h]\n",
        "      mae = mean_absolute_error(yt, yp)\n",
        "      rmse = np.sqrt(mean_squared_error(yt, yp))\n",
        "      r2 = r2_score(yt, yp)\n",
        "      denom = np.sum((yt - np.mean(yt))**2)\n",
        "      nse = 1 - np.sum((yt-yp)**2)/denom if denom!=0 else np.nan\n",
        "      rows.append({'horizon': h+1, 'MAE': mae, 'RMSE': rmse, 'R2': r2, 'NSE': nse})\n",
        "    return pd.DataFrame(rows), Y_inv, P_inv"
      ],
      "metadata": {
        "id": "_M1mlWc_zm1N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Pipeline manager: tiện lợi để gọi\n",
        "# ----------------------------\n",
        "class PipelineManager:\n",
        "  def __init__(self, cfg: Config):\n",
        "    self.cfg = cfg\n",
        "    os.makedirs(os.path.dirname(cfg.cleaned_data_path), exist_ok=True)\n",
        "    os.makedirs(cfg.models_dir, exist_ok=True)\n",
        "    self.processor = DataProcessor(cfg)\n",
        "    self.trainer = Trainer(cfg, scaler_path=cfg.scaler_path if os.path.exists(cfg.scaler_path) else None)\n",
        "\n",
        "  def prepare(self):\n",
        "    df_scaled = pd.read_parquet(self.cfg.cleaned_data_path) if os.path.exists(self.cfg.cleaned_data_path) else self.processor.run()\n",
        "    scaler = joblib.load(self.cfg.scaler_path) if os.path.exists(self.cfg.scaler_path) else self.processor.scaler\n",
        "    return df_scaled, scaler\n",
        "\n",
        "  def create_loaders(self, df_scaled: pd.DataFrame, lookback: int, horizon: int):\n",
        "    n = len(df_scaled)\n",
        "    train_n = int(n * (1 - self.cfg.val_size_ratio - self.cfg.test_size_ratio))\n",
        "    val_n = int(n * self.cfg.val_size_ratio)\n",
        "    train_df = df_scaled.iloc[:train_n]\n",
        "    val_df = df_scaled.iloc[train_n:train_n+val_n]\n",
        "    test_df = df_scaled.iloc[train_n+val_n:]\n",
        "    tr_ds = TimeSeriesDataset(train_df, lookback, horizon, self.cfg.pred_col)\n",
        "    va_ds = TimeSeriesDataset(val_df, lookback, horizon, self.cfg.pred_col)\n",
        "    te_ds = TimeSeriesDataset(test_df, lookback, horizon, self.cfg.pred_col)\n",
        "    tr = DataLoader(tr_ds, batch_size=self.cfg.batch_size, shuffle=True)\n",
        "    va = DataLoader(va_ds, batch_size=self.cfg.batch_size, shuffle=False)\n",
        "    te = DataLoader(te_ds, batch_size=self.cfg.batch_size, shuffle=False)\n",
        "    return tr, va, te\n",
        "\n",
        "  def save_model(self, model: nn.Module, name: str):\n",
        "    path = os.path.join(self.cfg.models_dir, name + '.pth')\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print('Saved model to', path)\n",
        "    return path"
      ],
      "metadata": {
        "id": "8PqDcT8z1LTn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo và Test"
      ],
      "metadata": {
        "id": "q_XV4cEh1D9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Demo + tests\n",
        "# ----------------------------\n",
        "def demo_full_pipeline(cfg: Config):\n",
        "  pm = PipelineManager(cfg)\n",
        "\n",
        "  # prepare data\n",
        "  df_scaled, scaler = pm.prepare()\n",
        "  print('Data scaled loaded shape:', df_scaled.shape)\n",
        "\n",
        "  # create loaders for Task1\n",
        "  tr1, va1, te1 = pm.create_loaders(df_scaled, cfg.lookback, cfg.horizon_task1)\n",
        "\n",
        "  # instantiate models\n",
        "  input_dim = len(cfg.features)\n",
        "  rnn = SimpleRNN(input_dim, hidden=64, num_layers=1, horizon=cfg.horizon_task1)\n",
        "  lstm = SimpleLSTM(input_dim, hidden=64, num_layers=1, horizon=cfg.horizon_task1)\n",
        "  cust = CustomizedLSTM(input_dim, hidden=128, num_layers=1, horizon=cfg.horizon_task1)\n",
        "  trainer = pm.trainer\n",
        "\n",
        "  # train small epochs for demo\n",
        "  print('Training SimpleRNN (demo)...')\n",
        "  rnn = trainer.train(rnn, tr1, va1, epochs=6, lr=1e-3, task=1, horizon=cfg.horizon_task1)\n",
        "  print('Training SimpleLSTM (demo)...')\n",
        "  lstm = trainer.train(lstm, tr1, va1, epochs=6, lr=1e-3, task=1, horizon=cfg.horizon_task1)\n",
        "  print('Training CustomizedLSTM (demo)...')\n",
        "  cust = trainer.train(cust, tr1, va1, epochs=6, lr=1e-3, task=1, horizon=cfg.horizon_task1)\n",
        "\n",
        "  # evaluate\n",
        "  print('Evaluate RNN:')\n",
        "  met_rnn, Y_rnn, P_rnn = trainer.evaluate(rnn, te1, cfg.horizon_task1, task=1, scaler=scaler)\n",
        "  print(met_rnn)\n",
        "\n",
        "  print('Evaluate LSTM:')\n",
        "  met_lstm, Y_lstm, P_lstm = trainer.evaluate(lstm, te1, cfg.horizon_task1, task=1, scaler=scaler)\n",
        "  print(met_lstm)\n",
        "\n",
        "  print('Evaluate Customized:')\n",
        "  met_cust, Y_cust, P_cust = trainer.evaluate(cust, te1, cfg.horizon_task1, task=1, scaler=scaler)\n",
        "  print(met_cust)\n",
        "\n",
        "  # save models\n",
        "  pm.save_model(rnn, 'simple_rnn_task1')\n",
        "  pm.save_model(lstm, 'simple_lstm_task1')\n",
        "  pm.save_model(cust, 'custom_lstm_task1')\n",
        "\n",
        "  # Task2: Seq2Seq\n",
        "  tr2, va2, te2 = pm.create_loaders(df_scaled, cfg.lookback, cfg.horizon_task2)\n",
        "  seq = Seq2SeqAttention(input_dim, enc_hidden=128, dec_hidden=128)\n",
        "  print('Training Seq2Seq (demo)...')\n",
        "\n",
        "  seq = trainer.train(seq, tr2, va2, epochs=8, lr=1e-3, task=2, horizon=cfg.horizon_task2)\n",
        "  met_seq, Y_seq, P_seq = trainer.evaluate(seq, te2, cfg.horizon_task2, task=2, scaler=scaler)\n",
        "  print('Seq2Seq metrics:')\n",
        "  print(met_seq)\n",
        "  pm.save_model(seq, 'seq2seq_task2')\n",
        "\n",
        "  print('Demo finished. Models and processed data saved.')\n",
        "\n",
        "# Unit tests for small parts\n",
        "def test_dataset_class(cfg: Config):\n",
        "  proc = DataProcessor(cfg)\n",
        "  df_scaled = proc.run()\n",
        "  # create small dataset\n",
        "  ds = TimeSeriesDataset(df_scaled.iloc[:200], cfg.lookback, 1, cfg.pred_col)\n",
        "  x, y = ds[0]\n",
        "  assert x.shape == (cfg.lookback, len(cfg.features))\n",
        "  assert y.shape[0] == 1\n",
        "  print('test_dataset_class passed.')\n",
        "\n",
        "def test_models_forward(cfg: Config):\n",
        "  proc = DataProcessor(cfg)\n",
        "  df_scaled = proc.run()\n",
        "  input_dim = len(cfg.features)\n",
        "  x = torch.randn(4, cfg.lookback, input_dim)\n",
        "  rnn = SimpleRNN(input_dim, horizon=cfg.horizon_task1)\n",
        "  lstm = SimpleLSTM(input_dim, horizon=cfg.horizon_task1)\n",
        "  print('test_models_forward passed.')"
      ],
      "metadata": {
        "id": "iRg81fiy1HOt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# If run as script, show sample usage\n",
        "# ----------------------------\n",
        "if __name__ == '__main__':\n",
        "  cfg = Config()\n",
        "  print('Config:', asdict(cfg))\n",
        "  # Run tests\n",
        "  print('Running lightweight unit tests...')\n",
        "  test_dataset_class(cfg)\n",
        "  test_models_forward(cfg)\n",
        "  # Run demo pipeline (small epochs) — uncomment to run demo\n",
        "  demo_full_pipeline(cfg)\n",
        "\n",
        "  print('Ready. Để chạy demo đầy đủ, call demo_full_pipeline(cfg) — chú ý tốn thời gian huấn luyện.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzF3-hkm2GB5",
        "outputId": "b09f00a8-a5ad-4c3c-a594-8763651b4895"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config: {'data_path': '/content/Metro_Interstate_Traffic_Volume.csv', 'pred_col': 'traffic_volume', 'features': ['traffic_volume', 'temp', 'rain_1h', 'snow_1h', 'clouds_all', 'hour', 'dayofweek'], 'lookback': 24, 'horizon_task1': 2, 'horizon_task2': 5, 'test_size_ratio': 0.15, 'val_size_ratio': 0.15, 'batch_size': 64, 'scaler_path': '/content/data/processed/scaler.save', 'cleaned_data_path': '/content/data/processed/cleaned_data.parquet', 'models_dir': '/content/data/models', 'device': 'cuda'}\n",
            "Running lightweight unit tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2899595364.py:35: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[needed] = df[needed].interpolate().fillna(method='bfill').fillna(method='ffill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cleaned scaled data to /content/data/processed/cleaned_data.parquet\n",
            "Saved scaler to /content/data/processed/scaler.save\n",
            "test_dataset_class passed.\n",
            "Saved cleaned scaled data to /content/data/processed/cleaned_data.parquet\n",
            "Saved scaler to /content/data/processed/scaler.save\n",
            "test_models_forward passed.\n",
            "Data scaled loaded shape: (48204, 7)\n",
            "Training SimpleRNN (demo)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2899595364.py:35: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df[needed] = df[needed].interpolate().fillna(method='bfill').fillna(method='ffill')\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6 train=0.023493 val=0.008800\n",
            "Epoch 2/6 train=0.011666 val=0.007056\n",
            "Epoch 3/6 train=0.010187 val=0.006878\n",
            "Epoch 4/6 train=0.009330 val=0.005641\n",
            "Epoch 5/6 train=0.008689 val=0.005774\n",
            "Epoch 6/6 train=0.008303 val=0.005184\n",
            "Training SimpleLSTM (demo)...\n",
            "Epoch 1/6 train=0.034783 val=0.010771\n",
            "Epoch 2/6 train=0.012705 val=0.007732\n",
            "Epoch 3/6 train=0.011056 val=0.007257\n",
            "Epoch 4/6 train=0.010292 val=0.006920\n",
            "Epoch 5/6 train=0.009690 val=0.006184\n",
            "Epoch 6/6 train=0.009152 val=0.006010\n",
            "Training CustomizedLSTM (demo)...\n",
            "Epoch 1/6 train=0.020142 val=0.009283\n",
            "Epoch 2/6 train=0.011274 val=0.008469\n",
            "Epoch 3/6 train=0.009993 val=0.007641\n",
            "Epoch 4/6 train=0.009177 val=0.006653\n",
            "Epoch 5/6 train=0.008600 val=0.005737\n",
            "Epoch 6/6 train=0.008090 val=0.005223\n",
            "Evaluate RNN:\n",
            "   horizon         MAE        RMSE        R2       NSE\n",
            "0        1  307.849164  430.818606  0.952848  0.952848\n",
            "1        2  416.666102  575.524407  0.915849  0.915849\n",
            "Evaluate LSTM:\n",
            "   horizon         MAE        RMSE        R2       NSE\n",
            "0        1  313.936089  441.344777  0.950516  0.950516\n",
            "1        2  425.920433  622.850905  0.901440  0.901440\n",
            "Evaluate Customized:\n",
            "   horizon         MAE        RMSE        R2       NSE\n",
            "0        1  318.619842  438.875528  0.951068  0.951068\n",
            "1        2  418.593957  573.998961  0.916294  0.916294\n",
            "Saved model to /content/data/models/simple_rnn_task1.pth\n",
            "Saved model to /content/data/models/simple_lstm_task1.pth\n",
            "Saved model to /content/data/models/custom_lstm_task1.pth\n",
            "Training Seq2Seq (demo)...\n",
            "Epoch 1/8 train=0.035590 val=0.018705\n",
            "Epoch 2/8 train=0.019135 val=0.015381\n",
            "Epoch 3/8 train=0.016401 val=0.010664\n",
            "Epoch 4/8 train=0.014674 val=0.010061\n",
            "Epoch 5/8 train=0.013609 val=0.009883\n",
            "Epoch 6/8 train=0.012809 val=0.009078\n",
            "Epoch 7/8 train=0.012308 val=0.008527\n",
            "Epoch 8/8 train=0.011835 val=0.008935\n",
            "Seq2Seq metrics:\n",
            "   horizon         MAE        RMSE        R2       NSE\n",
            "0        1  301.004691  428.353625  0.953398  0.953398\n",
            "1        2  387.779732  552.727065  0.922388  0.922388\n",
            "2        3  447.981204  662.283254  0.888566  0.888566\n",
            "3        4  493.010619  759.550546  0.853438  0.853438\n",
            "4        5  543.125324  826.783103  0.826358  0.826358\n",
            "Saved model to /content/data/models/seq2seq_task2.pth\n",
            "Demo finished. Models and processed data saved.\n",
            "Ready. Để chạy demo đầy đủ, call demo_full_pipeline(cfg) — chú ý tốn thời gian huấn luyện.\n"
          ]
        }
      ]
    }
  ]
}